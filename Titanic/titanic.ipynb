{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from the disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "### Background\n",
    "- O Titanic foi um famoso navio britânico que afundou no ano de 1912 após atingir um iceberg.\n",
    "- Haviam 2224 pessoas a bordo, 1500 morreram no desastre\n",
    "\n",
    "### Problemática\n",
    "- Através dos dados fornecidos prever quais passageiros sobreviveriam no desastre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviando solução\n",
    "- Enviar arquivo csv com 418 registros, mais o cabeçalho\n",
    "- Arquivo deve conter somente 2 colunas\n",
    " - PassengerId (Qualquer ordem)\n",
    " - Survived (1 para sobrevivente, 0 para vítima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victo\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Locais \n",
    "from libs.fastai.imports import *\n",
    "from libs.fastai.structured import *\n",
    "from libs import ml_helper\n",
    "\n",
    "#####################################################\n",
    "# Libs instaladas na venv do anaconda\n",
    "# --> Pré-processamento de dados\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# --> Análise de testes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# --> Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.getcwd()) + '\\\\data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_train = pd.read_csv(f'{PATH}\\\\train.csv', low_memory = False)\n",
    "df_raw_test  = pd.read_csv(f'{PATH}\\\\test.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context('display.max_rows', 1000, 'display.max_columns', 1000):\n",
    "        IPython.display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_with_mean(_df, _column):\n",
    "    valores_column = pd.unique(_df[_column])\n",
    "    valor_medio = round(np.nanmean(valores_column), 0)\n",
    "    _df[_column] = _df[_column].fillna(valor_medio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_with_mean(df_raw_train, 'Age')\n",
    "fill_na_with_mean(df_raw_test, 'Age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processor(df):\n",
    "    \n",
    "    \n",
    "    Embarked = set(df.loc[df[\"Embarked\"].notnull()][\"Embarked\"])\n",
    "    Embarked_to_number = {ni: indi for indi, ni in enumerate(set(Embarked))}\n",
    "    df['Embarked'] = df['Embarked'].map(Embarked_to_number)\n",
    "    \n",
    "    df['Age'].fillna(df['Age'].median(), inplace = True)\n",
    "    df['Embarked'].fillna(df['Embarked'].median(), inplace = True)\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace = True)\n",
    "    \n",
    "    df['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    title_names = (df['Title'].value_counts() < 10)\n",
    "    df['Title'] = df['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    df.Sex = labelencoder.fit_transform(df.Sex)\n",
    "    \n",
    "    is_sector_ticket_number = lambda x: x[1] if len(x) > 1 else ''\n",
    "    lambda_ticket_number = lambda x: x[0] if x[0].isdigit() else is_sector_ticket_number(x)    \n",
    "    df.Ticket_Number = [lambda_ticket_number(x) for x in df.Ticket.str.split(' ')]\n",
    "\n",
    "    df = df.drop(['Ticket', 'Name', 'Cabin'], axis = 1)\n",
    "    \n",
    "    train_cats(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pd.get_dummies(df)\n",
    "   # return pd.get_dummies(df, prefix=['PClass', 'Sex', 'SibSP', 'Parch', 'Ticket'], columns= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "df_raw_train = pre_processor(df_raw_train)\n",
    "df_raw_test = pre_processor(df_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando training set para arquivo feather\n",
    "os.makedirs('tmp', exist_ok =True)\n",
    "df_raw_train.to_feather('tmp/titanic_train')\n",
    "\n",
    "# Salvando training set para arquivo feather\n",
    "df_raw_test.to_feather('tmp/titanic_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeiros testes do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_feather('tmp/titanic_train')\n",
    "df_test = pd.read_feather('tmp/titanic_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, nas_train = proc_df(df_train, 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestClassifier(n_estimators = 100, max_depth = 5, n_jobs=-1)\n",
    "m.fit(x_train, y_train)\n",
    "m.score(x_train, y_train)\n",
    "predictions = m.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428731762065096\n"
     ]
    }
   ],
   "source": [
    "print(m.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando dentre algorítmos de classificação, qual é o que possui o melhor indicador para focalizar esforços\n",
    "## O procedimento a seguir foi retirado de um notebook do kaggle de jeffd23\n",
    "link: *https://www.kaggle.com/jeffd23/10-classifier-showdown-in-scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.3, random_state =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 58.9552%\n",
      "Log Loss: 6.182371454696778\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 57.0896%\n",
      "Log Loss: 0.6342387462617194\n",
      "==============================\n",
      "NuSVC\n",
      "****Results****\n",
      "Accuracy: 76.1194%\n",
      "Log Loss: 0.5360770542400297\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 73.8806%\n",
      "Log Loss: 9.021322192700552\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 78.3582%\n",
      "Log Loss: 0.6156812398007528\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 77.2388%\n",
      "Log Loss: 0.6783896580309434\n",
      "==============================\n",
      "GradientBoostingClassifier\n",
      "****Results****\n",
      "Accuracy: 77.2388%\n",
      "Log Loss: 0.500825945161359\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 77.2388%\n",
      "Log Loss: 1.4089901646498368\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 79.1045%\n",
      "Log Loss: 0.5525765549702472\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 55.2239%\n",
      "Log Loss: 12.904542190486456\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victo\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(x_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(x_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(x_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizando o processo de teste\n",
    "## O procedimento a seguir foi retirado do notebook do kaggle de westen30\n",
    "link: *https://www.kaggle.com/westen30/titanic-data-v2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o melhor resultado possível alterando os hyperparemeters de RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:  5.2min finished\n"
     ]
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler())]\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Criando o grid de parametros\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [RandomForestClassifier(random_state=1)],\n",
    "     'classifier__n_estimators' : [50, 100, 200, 500],\n",
    "     'classifier__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "     # 'classifier__min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "     'classifier__max_depth': [3,4,5,6,7],\n",
    "     'classifier__oob_score': [True, False],\n",
    "     'classifier__criterion': ['gini', 'entropy']}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Criando o grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 10, verbose=True, n_jobs = -1, scoring= 'accuracy')\n",
    "\n",
    "# Treinando os dados\n",
    "\n",
    "best_clf = clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723880597014925\n",
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                                        class_weight=None, criterion='gini',\n",
      "                                        max_depth=7, max_features='auto',\n",
      "                                        max_leaf_nodes=None, max_samples=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=True, random_state=1,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=7, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=True, random_state=1, verbose=0,\n",
      "                       warm_start=False), 'classifier__criterion': 'gini', 'classifier__max_depth': 7, 'classifier__max_features': 'auto', 'classifier__n_estimators': 100, 'classifier__oob_score': True}\n",
      "0.8524065540194572\n"
     ]
    }
   ],
   "source": [
    "best_prediction = best_clf.predict(x_test)\n",
    "print(best_clf.score(x_test, y_test))\n",
    "print(best_clf.best_estimator_)\n",
    "print(best_clf.best_params_)\n",
    "print(best_clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o melhor resultado possível alterando os hyperparameters de GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1620 candidates, totalling 16200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 498 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1298 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2498 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3248 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5048 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6098 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7248 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8498 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9848 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11298 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12848 tasks      | elapsed: 23.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14498 tasks      | elapsed: 32.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16200 out of 16200 | elapsed: 40.8min finished\n"
     ]
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler())]\n",
    "pipe_grad_boosting = Pipeline([('scaler', StandardScaler()), ('classifier', GradientBoostingClassifier())])\n",
    "\n",
    "# Criando o grid de parametros\n",
    "\n",
    "param_grid_grad_boosting = [\n",
    "    {'classifier': [GradientBoostingClassifier(random_state=2)],\n",
    "     'classifier__learning_rate': [1.0, 0.5, 0.1],\n",
    "     'classifier__n_estimators' : [50, 100, 200, 500],\n",
    "     'classifier__subsample': [1.0, 0.5, 0.1],\n",
    "     'classifier__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "     'classifier__max_depth': [3,4,5,6,7],\n",
    "     'classifier__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Criando o grid search object\n",
    "\n",
    "clf_grad_boosting = GridSearchCV(pipe, param_grid = param_grid_grad_boosting, cv = 10, verbose=True, n_jobs = -1, scoring= 'accuracy')\n",
    "\n",
    "# Treinando os dados\n",
    "\n",
    "best_clf_grad_boostin = clf_grad_boosting.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611940298507462\n",
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(ccp_alpha=0.0, criterion='mae',\n",
      "                                            init=None, learning_rate=0.1,\n",
      "                                            loss='deviance', max_depth=5,\n",
      "                                            max_features='auto',\n",
      "                                            max_leaf_nodes=None,\n",
      "                                            min_impurity_decrease=0.0,\n",
      "                                            min_impurity_split=None,\n",
      "                                            min_samples_leaf=1,\n",
      "                                            min_samples_split=2,\n",
      "                                            min_weight_fraction_leaf=0.0,\n",
      "                                            n_estimators=50,\n",
      "                                            n_iter_no_change=None,\n",
      "                                            presort='deprecated',\n",
      "                                            random_state=2, subsample=0.5,\n",
      "                                            tol=0.0001, validation_fraction=0.1,\n",
      "                                            verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "{'classifier': GradientBoostingClassifier(ccp_alpha=0.0, criterion='mae', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "                           max_features='auto', max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=2, subsample=0.5, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False), 'classifier__criterion': 'mae', 'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__n_estimators': 50, 'classifier__subsample': 0.5}\n",
      "0.8524065540194572\n"
     ]
    }
   ],
   "source": [
    "best_clf_grad_boostin\n",
    "\n",
    "best_prediction_grad_boosting = best_clf_grad_boostin.predict(x_test)\n",
    "print(best_clf_grad_boostin.score(x_test, y_test))\n",
    "print(best_clf_grad_boostin.best_estimator_)\n",
    "print(best_clf_grad_boostin.best_params_)\n",
    "print(best_clf_grad_boostin.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_best_clf(clf, _name):\n",
    "        clf_pred = clf.predict(df_test)\n",
    "        output_clf = pd.DataFrame({'PassengerId' : df_test.PassengerId, 'Survived' : clf_pred})\n",
    "        output_clf.to_csv(f'{_name}.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_best_clf(best_clf,'submit_rand_forest')\n",
    "predict_best_clf(best_clf_grad_boostin,'submit_grad_boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
